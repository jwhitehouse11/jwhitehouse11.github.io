

  <!DOCTYPE html>
<html>
   <head>
      <title>Justin Whitehouse</title>
      <style type="text/css">
         a {
         text-decoration: none;
         }
         li {
         margin-left: 0;
         margin-right: 10%;
         margin-top: .6em;
         margin-bottom: .6em;
         }


         a.author-link:link, a.author-link:active, a.author-link:visited {color: #000000; text-decoration:none;}
         a.author-link:hover {text-decoration: underline;}

      </style>
   </head>
   <body text="black" bgcolor="#FFFFFF"
      link="red" vlink="purple" alink="red" >
      <table cellpadding="20">
         <tr>
            <td>
               <img src="justin_whitehouse.jpg" height="250" style="border:2px solid black">
            </td>
            <td>
               <font size="5"><strong>Justin Whitehouse</strong></font>
               <br><br>Email: jwhiteho (at) andrew (dot) cmu (dot) edu
               <!--<br><br>CV: <a href="CV_Justin_Whitehouse_site.pdf">click here</a>-->
            </td>
         </tr>
      </table>
      <p>
         I am a rising fifth year PhD student in the Computer Science Department
          at Carnegie Mellon University. I am currently co-advised by <a href="http://stat.cmu.edu/~aramdas/">Aaditya Ramdas</a> and <a href="https://zstevenwu.com/">Steven Wu</a>. On the theoretical side, I am interested in studying the growth of self-normalized processes in high or infinite-dimensional settings. Practically, I am interested in studying how self-normalized concentration can facilitate adaptive data analysis, with particular interest in applications in causal inference, online learning, and private machine learning (see papers below).
          <br><br>

          Before working in the above areas, I studied problems in stochastic scheduling and queueing with <a href="http://www.cs.cmu.edu/~harchol/">Mor Harchol-Balter</a> and <a href="http://cs.cmu.edu/~weinaw/">Weina Wang</a>. In particular, we developed optimal algorithms for scheduling parallelizable jobs in multiserver systems.
         <br><br>

         Before coming to Carnegie Mellon, I was an undergraduate at Columbia University in New York City. There, I majored in mathematics and computer science. I was fortunate enough to be advised by <a href="https://www.thecomputersciencecomedian.com/">Allison Bishop</a> and <a href="http://www.cs.columbia.edu/~suman/">Suman Jana </a>.
      </p>
      <hr />
      <h3>Publications and Preprints</h3>
      <div align="left">
         <ul>
           <li><a href="arxiv_2023_bandit.pdf">Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB</a>
            (with <a class="author-link" href="http://stat.cmu.edu/~aramdas/">Aaditya Ramdas</a> and <a class="author-link" href="https://zstevenwu.com/">Steven Wu</a>). <br/>
            Arxiv Preprint.
            </li>
           <li><a href="arxiv_2023_pcr.pdf">Adaptive Principal Component Regression with Applications to Panel Data</a> 
            (with <a class="author-link" href="https://sites.google.com/view/anishagarwal/home">Anish Agarwal</a>, <a class="author-link" href="https://keeganharris.github.io/">Keegan Harris</a>, and <a class="author-link" href="https://zstevenwu.com/">Steven Wu</a>). <br/>
            Arxiv Preprint.
           </li>
           <li><a href="icml_2023_adaptive.pdf">Fully-Adaptive Composition in Differential Privacy</a>
            (with <a class="author-link" href="http://stat.cmu.edu/~aramdas/">Aaditya Ramdas</a>, <a class="author-link" href="https://zstevenwu.com/">Steven Wu</a>, and <a class="author-link" href="https://www.linkedin.com/in/rrogers386/">Ryan Rogers</a>). <br/>
              ICML, 2023.             
           </li>
           <li><a href="neurips_2022_brownian.pdf">Brownian Noise Reduction: Maximizing Privacy Subject to Accuracy Constraints</a>
            (with <a class="author-link" href="http://stat.cmu.edu/~aramdas/">Aaditya Ramdas</a>, <a class="author-link" href="https://zstevenwu.com/">Steven Wu</a>, and <a class="author-link" href="https://www.linkedin.com/in/rrogers386/">Ryan Rogers</a>). <br/>
              Neurips, 2022.
           </li>
           <li><a href="performance_2021.pdf">The Case for Phase-Aware Scheduling of Parallelizable Jobs</a>
            (with <a class="author-link" href="http://bsb20.github.io/">Benjamin Berg</a>, <a class='author-link' href="http://www.andrew.cmu.edu/user/moseleyb/">Benjamin Moseley</a>, <a class='author-link' href="http://www.cs.cmu.edu/~harchol/">Mor Harchol-Balter</a>, and <a class='author-link' href="http://cs.cmu.edu/~weinaw/">Weina Wang</a>). <br/>
              39th International Symposium on Computer Performance, Modeling, Measurements and Evaluation, 2021.
           <li><a href="spaa_2020.pdf">Optimal Resource Allocation for Elastic and Inelastic Jobs</a>
             (with <a class="author-link" href="http://bsb20.github.io/">Benjamin Berg</a>, <a class='author-link' href="http://www.andrew.cmu.edu/user/moseleyb/">Benjamin Moseley</a>, <a class='author-link' href="http://www.cs.cmu.edu/~harchol/">Mor Harchol-Balter</a>, and <a class='author-link' href="http://cs.cmu.edu/~weinaw/">Weina Wang</a>). <br/>
              ACM Symposium on Parallelism in Algorithms and Architectures (SPAA 2020).
           </li>
            <li><a href="sigops_2019.pdf">
              Bringing Engineering Rigor to Deep Learning</a>
              (with Kexin Pei, Shiqi Wang, Yuchi Tian, Carl Vondrick, Yinzhi Cao, Baishakhi Ray, Suman Jana, and Junfen Yang).
              <br/>
               ACM SIGOPS Operating Systems Review, Volume 53 Issue 1 (SIGOPS 2019).

            </li>
            <li><a href="neurips_2018.pdf">
              Efficient Formal Safety Analysis of Neural Networks</a>
              (with <a class="author-link" href="https://www.cs.columbia.edu/~tcwangshiqi/">Shiqi Wang</a>, <a class='author-link' href="http://www.cs.columbia.edu/~suman/" > Suman Jana</a>, <a class='author-link' href='https://sites.google.com/site/kexinpeisite/'> Kexin Pei</a>, and <a class='author-link' href="http://www.cs.columbia.edu/~junfeng/"> Junfeng Yang</a>).
              <br/>
               Neurips, 2018.

            </li>
            <li><a href="usenix_2018.pdf">
            Formal Security Analysis of Neural Networks Using Symbolic Intervals</a>
            (with <a class="author-link" href="https://www.cs.columbia.edu/~tcwangshiqi/">Shiqi Wang</a>, <a class='author-link' href="http://www.cs.columbia.edu/~suman/" > Suman Jana</a>, <a class='author-link' href='https://sites.google.com/site/kexinpeisite/'> Kexin Pei</a>, and <a class='author-link' href="http://www.cs.columbia.edu/~junfeng/"> Junfeng Yang</a>).
          </br>
            27th USENIX Security Symposium, 2018.
            </li>
         </ul>
      </div>
      </div>
      <h3>Teaching</h3>
      I have served as a teaching assistant for the following classes.
      <div align="left">
         <ul>
            <li> Graduate Algorithms (Spring 2022, CMU).
            </li>
            <li> Foundations of Privacy (Fall 2021, CMU).
            </li>
            <li> Computer Science Theory (Spring 2019, Columbia).
            </li>
            <li> Modern Algebra II (Spring 2019, Columbia).
            </li>
            <li> Complexity Theory (Fall 2018, Columbia).
            </li>
            <li> Introduction to Cryptography (Fall 2018, Columbia).
            </li>
            <li> Number Theory and Cryptography (Spring 2018, Columbia).
            </li>
         </ul>
      </div>
   </body>
</html>
